---
title: "Buzzing Memory component analysis and Model Validation"
author: "Susanne Ditlevsen and Adeline Samson"
date: "07/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##  

Tervo OM, Blackwell SB, Ditlevsen S, Conrad A, Samson AL, Garde E, Hansen RG and Heide-Jørgensen MP. Narwhals react to ship noise and airgun pulses embedded in background noise"

Reference for the memory kernel: Aleksander Søltoft-Jensen, Mads Peter Heide-Jørgensen and Susanne Ditlevsen: Modelling the sound production of narwhals using a point process framework with memory effects. Annals of Applied Statistics, 14(4), 2037-2052, 2020.

```{r}
library(splines)
library(lme4)
library(ggplot2)
library(data.table)  ## For fast and easy reading of large data sets


## The two functions used in the analysis ##

TimeFunction <- function(n, dt = 1){
  ## n: Number of observations. Positive integer
  ## dt: Time step between observations in seconds
  Time <- (1:n)/(3600/dt) ## Time in hours since tagging
}


ExposureFunction <- function(X){
  ## X: Distance to ship in meters measured each second. Supposed to be positive
  
  ## Exposure variable
  Xtilde <- 1000/X ## Inverse of distance to ship in kilometers
  Xtilde[is.na(Xtilde)] <- 0 ## If not in line of sight, exposure is zero
  Xtilde
}
```

```{r, warning=FALSE, echo = FALSE}
dataBioLet <- fread(file="data_BiolLet_Tervo_etal_2021.txt")

dataAll <- fread("~/Dropbox/Effect study EGRL 2018/Database/db_narwhal_2017_2018.txt")
data <- dataAll


## Time provides time since tagging
data$Time <- numeric()
for(ind in unique(data$Ind)){
  indicator <- (data$Ind == ind)
  data$Time[indicator] <- TimeFunction(n = sum(indicator))
}

## Define the first 100 time lags
temp <- as.data.frame(shift(data$Buzz, n = 1:100, give.names = TRUE))
data <- cbind(temp, data)

## Find times of first buzz and first exposure time and 
## restrict data to be between these for each whale
for(k in unique(data$Ind)){
  mintime <- min(data$Time[(data$Ind == k) & (data$Buzz == 1)])
  maxtime <- min(data$Time[(data$Ind == k) & (data$LOS == 1)])
  data <- subset(data, !(Ind == k & Time > maxtime))
  data <- subset(data, !(Ind == k & Time < mintime))
}

## keep the 2018 data
data = data[data$Year=="2018",]
```


## 1. Estimation with a glm model without Depth

#### Selection of the number of memory components

In the following the memory effect of buzzing is determined for the whales of the effect study, data from 2018. The idea is as follows: 1) Restrict the data to the 6 whales from 2018 before first exposure to capture natural behavior and 2) Estimate the memory component (autoregressive components because there is autocorrelation in the buzzing activity) in a Poisson glm (all whales pooled, no individual effects). This common estimate of the memory kernel is then to be used in a random effects model as an offset to evaluate exposure effects. 

We fitted a memory component to buzzes with lags from 1 to $k$, where we varied $k$ between 1 and 100. For each model, we computed the BIC and chose the model with the lowest BIC. Below, only a subset of k are chosen. This can be changed as desired in the definition of lagvector.

```{r, warning=FALSE, echo = FALSE}
LagVariables <- names(data[,1:100])

glmresults <- list(NULL)

lagvector <- c(10,20,30,40,50,seq(60,70,1),80, 90, 100)
lagvector <- c(50,seq(60,70,1))

n <- length(lagvector)
AICvector <- numeric(n) 
BICvector <- numeric(n) 
for(i in 1:n){
  form <- paste("Buzz ~ Ind + ",
                paste(LagVariables[1:lagvector[i]], collapse = " + "))
  
  glmAllBuzz <- glm(form, 
                    data = data, 
                    family = poisson)
  
  AIC <- AIC(glmAllBuzz)
  BIC <- BIC(glmAllBuzz)
  
  AICvector[i]    <- AIC
  BICvector[i]    <- BIC
  coeftable <- coefficients(summary(glmAllBuzz))
  glmresults[[i]] <- list(lag = lagvector[i], coeftable = coeftable, AIC = AIC, BIC = BIC)
}
```

The following figure shows the BIC values for different maximum lag values. The minimum BIC is for maximum lag = 63.

```{r, warning=FALSE, echo = FALSE, fig.width=8}
dataBIC <- data.frame(lag = lagvector, BIC = BICvector)

ggplot(dataBIC, aes(x = lag, y = BIC)) +
  geom_line() +
  geom_vline(xintercept = 63, col = "blue", lty = 2) +
  xlab("Maximum lag")
```


The following figure shows the fitted memory components for each lag for different maximum lag values. The points are the individual estimates for each lag, the lines are the fitted double exponential curve used in the final determination of the memory component. Note that the estimates at a given lag change when the maximum lag is changed. This is because if the maximum lag is too small, the earlier lag estimates compensate for the lack of autoregressive effects for larger lags.

```{r, warning=FALSE, echo = FALSE, fig.width=8}
k <- length(glmresults)
kvector <- 1:12#c(2:6,16:19)

temp    <- glmresults[[1]]
ARcoef  <- temp$coeftable[-(1:6), "Estimate"]
lag     <- temp$lag

fitresult <- data.frame(maxlag = as.factor(rep(lag, lag)), lag = 1:lag,
                        estimate = ARcoef)

curvefit  <- data.frame(maxlag = as.factor(rep(lag, lag)), lag = 1:lag,
                        b1 = rep(NA, lag), b2 = rep(NA, lag), 
                        b3 = rep(NA, lag), b4 = rep(NA, lag))

for(i in kvector){
  temp    <- glmresults[[i]]
  lag     <- temp$lag
  ARcoef  <- temp$coeftable[-(1:6), "Estimate"]
  std.err <- temp$coeftable[-(1:6),"Std. Error"]
  w       <- 1/std.err
  n       <- length(ARcoef)
  
  fm <- nls(ARcoef ~ SSbiexp(1:n, A1, lrc1, A2, lrc2), weights = w)
  b1 <- summary(fm)$coefficients[1,1]
  b2 <- exp(summary(fm)$coefficients[2,1])
  b3 <- summary(fm)$coefficients[3,1]
  b4 <- exp(summary(fm)$coefficients[4,1])
  fitresulti <- data.frame(maxlag = as.factor(rep(lag, lag)), lag = 1:lag, 
                           estimate = ARcoef)
  curvefiti  <- data.frame(maxlag = as.factor(rep(lag, lag)), lag = 1:lag,
                           b1 = rep(b1, lag), b2 = rep(b2, lag), 
                           b3 = rep(b3, lag), b4 = rep(b4, lag))
  fitresult <- rbind(fitresult, fitresulti)
  curvefit  <- rbind(curvefit, curvefiti)
}

curvefit$estimate <- curvefit$b1 * exp(-curvefit$b2 * curvefit$lag) +
  curvefit$b3 * exp(-curvefit$b4 * curvefit$lag)

ggplot(fitresult, aes(x = lag, y = estimate, color = maxlag)) +
  geom_point(alpha = 0.3) +
  geom_line(aes(x = lag, y = estimate, color = maxlag), data = curvefit) +
  geom_hline(yintercept = 0) + 
  theme(legend.position="top")


```

 Estimation with a glmer and individual random effects does not run. 

#### Estimation of the exposure effect

```{r, warning=FALSE, echo = FALSE}
data <- dataAll

## Time provides time since tagging
data$Time <- numeric()
for(ind in unique(data$Ind)){
  indicator <- (data$Ind == ind)
  data$Time[indicator] <- TimeFunction(n = sum(indicator))
}

## Define the first 100 time lags
temp <- as.data.frame(shift(data$Buzz, n = 1:100, give.names = TRUE))
data <- cbind(temp, data)


## We cut the first 24 hours of all individuals to avoid
## the effect of tagging, i.e., cut 43200 data points
data <- data[data$Time > 24,]

## Defining exposures
data$Xtilde <- ExposureFunction(X = data$Dist_Ship) ## Exposure variable based on 1/distance in km

data$X = data$Xtilde*(data$Seismik == 1)
data$P = data$Xtilde*(data$Seismik == 0)

## Only data from airgun or no exposure at all (removing ship):
data <- subset(data, P == 0)

## Only data from 2018
data = data[data$Year=="2018",]

plotdist <- seq(2,50,0.1) # distances for plotting in km

## Find times of first exposure time and restrict data 
## to be before unless exposed to seismic for each whale
#for(k in unique(data$Ind)){
#  maxtime <- min(data$Time[(data$Ind == k) & (data$LOS == 1)])
#  data <- subset(data, !(Ind == k & Time > maxtime) | (Ind == k & X > 0))
#}

```


The generalized linear mixed model with a Poisson response distribution with a log-link to model the effect of exposure on buzzing rate (buzzes/min) with an autoregressive memory component. Exposure is defined as 1/distance (km).Exposure is entered non-linearly as an explanatory variable using natural cubic splines with 3 degrees of freedom (ns, package splines) with internal knots located at the 33th and 66th percentiles of the non-zero exposure values. Individual is included as a random effect allowing each animal to have a unique baseline (intercept) in their sound production rate. To obtain convergence the optimization is done by Adaptive Gauss-Hermite Quadrature, which is obtained by the option nAGQ = 0 in the glmer-call. The default is nAGQ = 1, the Laplace approximation, which does not reach convergence.


```{r, warning=FALSE, echo = FALSE}
library(lme4)
library(splines)

print("Coefficients in memory kernel")
b1 = -7.76826061  
b2 = 0.30956649  
b3 = 0.95340294  
b4 = 0.010943783
ARvec <- b1*exp(-b2*(1:63))+b3*exp(-b4*(1:63))

LagVariables <- names(data[,1:63])
dataAR  <- data[,LagVariables]
## Autoregressive component for offset in later analyses
data$AR <- as.matrix(dataAR) %*% ARvec

## Weights for the glmer analysis
data$n <- rep(0, length(data$Ind))
for(k in unique(data$Ind)){
  data$n[data$Ind == k] <- length(data$Ind[data$Ind == k])
}

glmerAllBuzz <- glmer(Buzz ~ offset(AR) +
                        ns(X, knots = quantile(data$X[data$X > 0],c(1:2)/3)) +
                        (1 | Ind),
                      data = data,
                      nAGQ=0,
                      weights = n,
                      family = poisson)

summary(glmerAllBuzz)
```





#### Model control for model with "only" AR offset (and no intercept in the offset)

We will do model control on our final model glmerAllBuzz.

We calculate uniform residuals (explained in the supplementary material) and make several plots: an autocorrelation plot that shows that residuals are approximately non-correlated,  a plot of residual $i$ against residual $i-1$ to see if any (unwanted) structure emerges, a qqplot and a prediction plot.


```{r, warning=FALSE, echo = FALSE, fig.width=8}

data$predict = predict(glmerAllBuzz, type = "response")

## Below we calculate the uniform residuals

Zall = list(NULL)
m = 1
#for(k in unique(data$Ind)){
for (k in unique(data$Ind)){
  datak = data[data$Ind == k,]
  n     = length(datak$Buzz)
  Z     = NULL  ## Uniform residuals
  Buzzindices = (1:n)[datak$Buzz == 1]
  nB          = length(Buzzindices)
  dataki      = datak[1:Buzzindices[1],]
  Z    = c(Z,(exp(-sum(dataki$predict))))
  for(i in 2:nB){
      dataki = datak[(Buzzindices[i-1]+1):(Buzzindices[i]),]
      Z    = c(Z, (1-exp(-sum(dataki$predict))))
  }
  dataki = datak[Buzzindices[nB]:n,]
  Z      = c(Z,(exp(-sum(dataki$predict))))
  Zall[[m]] = list(Z = Z, Ind = k)
  m = m + 1
}



## Collect the uniform residuals in a data.frame
Zdata = data.frame(Z = Zall[[1]]$Z, Ind = Zall[[1]]$Ind, 
                   n = length(Zall[[1]]$Z))
for(i in 2:6){
  Zdata = rbind(Zdata, 
                data.frame(Z = Zall[[i]]$Z, Ind = Zall[[i]]$Ind, 
                           n = length(Zall[[i]]$Z)))
}

nn = length(Zdata$Z)

temp = acf(Zdata$Z, plot = FALSE)

acfplotdata = data.frame(acf = temp$acf, lag = temp$lag)

ggplot(data = acfplotdata, aes(x = lag, y = acf)) +
  geom_col(size = 0.2) + coord_fixed(36) +
  ylab("Autocorrelation of uniform residuals") +
  xlab("Lag")

#ggsave("Zacf.png", width = 10, height = 8, units = "cm")

zplotdata = data.frame(Zlow = Zdata$Z[1:(nn-1)], Zupp = Zdata$Z[2:nn])

ggplot(data = zplotdata, aes(x = Zlow, y = Zupp)) +
  geom_point(size = 0.5) + coord_fixed(1) +
  xlab(expression(Z[i-1])) +
  ylab(expression(Z[i])) +
  geom_abline(slope = 1, intercept = 0, size = 1)
#ggsave("Zplot.png", width = 10, height = 8, units = "cm")
```


QQplot for a given depth

```{r, warning=FALSE, echo = FALSE, fig.width=8}
Zall = list(NULL)
m = 1
for(k in unique(data$Ind)){
  datak = data[data$Ind == k,]
  n     = length(datak$Buzz)
  Z     = NULL  ## Uniform residuals
  Depthk = NULL
  Xk = NULL
  Pk = NULL
  Buzzindices = (1:n)[datak$Buzz == 1]
  nB          = length(Buzzindices)
  for(i in 2:nB){
      dataki = datak[(Buzzindices[i-1]+1):(Buzzindices[i]),]
      Z    = c(Z, (1-exp(-sum(dataki$predict))))
      Depthk = c(Depthk, dataki$Depth[1])
      Xk     = c(Xk, dataki$X[1])
      Pk     = c(Pk, dataki$P[1])
  }
 Zall[[m]] = list(Z = Z, Ind = k, X = Xk, P = Pk , Depth = Depthk)
  m = m + 1
}


## Collect the uniform residuals in a data.frame
Zdata = data.frame(Z = Zall[[1]]$Z, 
                   Ind = Zall[[1]]$Ind, 
                   n = length(Zall[[1]]$Z), 
                   Depth = Zall[[1]]$Depth,
                   X = Zall[[1]]$X, P = Zall[[1]]$P)
for(i in 2:6){
  Zdata = rbind(Zdata, 
                data.frame(Z = Zall[[i]]$Z, 
                   Ind = Zall[[i]]$Ind, 
                   n = length(Zall[[i]]$Z), 
                   Depth = Zall[[i]]$Depth,
                   X = Zall[[i]]$X, P = Zall[[i]]$P))
}

ZdataDepthbt = Zdata[-Zdata$Depth>400 & Zdata$X==0 &Zdata$P==0, ]
#ZdataDepthbt = Zdata[ Zdata$X==0 &Zdata$P==0, ]
nresid = length(ZdataDepthbt$Z)
Zorder = ZdataDepthbt$Z[order(ZdataDepthbt$Z)]
plot((1:nresid)/nresid, Zorder)
abline(a=0,b=1)



```


Prediction plot (Figure 7)

```{r, warning=FALSE, echo = FALSE, fig.width=8}
Intensityall = list(NULL)
scale = 3600/6
m = 1
for(k in unique(data$Ind)){
  datak     = data[data$Ind == k,]
  n         = dim(datak)[1]
  nBuzz     = NULL
  Intensity = NULL
  Depthk    = NULL
  Xk        = NULL
  Pk        = NULL
  Expmax    = NULL
  Expmin    = NULL
  for (i in 1:floor(n/scale)){
    dataki      = datak[(i-1)*scale+1:scale,]
    Buzzindices = (1:scale)[dataki$Buzz == 1]
    nBuzz       = c(nBuzz, length(Buzzindices))
    Intensity   = c(Intensity, sum(dataki$predict))
    Depthk      = c(Depthk, dataki$Depth[1])
    Xk          = c(Xk, mean(dataki$X))
    Pk          = c(Pk, mean(dataki$P))
    Expmax      = c(Expmax, ifelse(mean(dataki$X)<0.05 &mean(dataki$P)<0.05, 0, 50)) 
    Expmin      = c(Expmin, 0)
}
 Intensityall[[m]] = list(nBuzz = nBuzz, Ind = k, Intensity = Intensity, X = Xk, P = Pk, Expmax = Expmax, Expmin= Expmin, Depth = Depthk)
  m = m + 1
}


## Collect the uniform residuals in a data.frame
Intensitydata = data.frame(nBuzz = Intensityall[[1]]$nBuzz, 
                   Ind = Intensityall[[1]]$Ind, 
                   Intensity = Intensityall[[1]]$Intensity, 
                   Time = 1:length(Intensityall[[1]]$nBuzz), 
                   Depth = Intensityall[[1]]$Depth,
                   X = Intensityall[[1]]$X*75, 
                   P = Intensityall[[1]]$P*75,
                   ExposureMax = Intensityall[[1]]$Expmax,
                   ExposureMin = Intensityall[[1]]$Expmin)

for(i in 2:6){
  Intensitydata = rbind(Intensitydata, 
                data.frame(nBuzz = Intensityall[[i]]$nBuzz, 
                   Ind = Intensityall[[i]]$Ind, 
                   Intensity = Intensityall[[i]]$Intensity, 
                   Time = 1:length(Intensityall[[i]]$nBuzz), 
                   Depth = Intensityall[[i]]$Depth,
                   X = Intensityall[[i]]$X*75, 
                   P = Intensityall[[i]]$P*75,
                   ExposureMax = Intensityall[[i]]$Expmax,
                   ExposureMin = Intensityall[[i]]$Expmin))
}
 




ggplot(Intensitydata, aes(x=Time, y=Intensity, group=Ind)) + 
   geom_ribbon(aes(ymin=ExposureMin, ymax=ExposureMax), fill = "#FFFFCC") + 
geom_line()+xlab("Hour")+ylim(0,50)+xlim(0,750)+
  geom_point(aes(x=Time, y=nBuzz), size=0.1, color="red")+
   facet_wrap(~Ind)
ggsave("BuzzCheckPredict.pdf") 

```
\newpage

## 2. Model with offset including AR estimated coefficients and estimated intercept 

We also consider a glmer analysis which keeps  the intercept estimated from the autoregressive glm model. This model is later called glmerAllBuzzIntercept.


```{r, warning=FALSE, echo = FALSE}
# Intercept: Fixed effect for individuals estimated from the previous model
IndIntercept = c(-5.3903184,
                 -5.3903184 -0.7349607,
                 -5.3903184 -0.4212932,
                 -5.3903184-0.5159544,
                 -5.3903184-0.2659106,
                 -5.3903184-0.1084244) 
data$ARIntercept <- data$AR
for (i in 1:6){
  data$ARIntercept[data$Ind==unique(data$Ind)[i]] = data$ARIntercept[data$Ind==unique(data$Ind)[i]] + IndIntercept[i]
}

glmerAllBuzzIntercept <- glmer(Buzz ~ offset(ARIntercept) +
                        ns(X, knots = quantile(data$X[data$X > 0],c(1:2)/3)) -1 +
                        (1 | Ind),
                      data = data,
                      nAGQ=0,
                      weights = n,
                      family = poisson)

summary(glmerAllBuzzIntercept)
```


#### Model control for model with  AR offset and estimated intercept offset

We will do model control on our final model glmerAllBuzzIntercept.

We calculate uniform residuals (explained in the supplementary material) and make four plots: an autocorrelation plot that shows that residuals are approximately non-correlated,  a plot of residual $i$ against residual $i-1$ to see if any (unwanted) structure emerges, a qqplot and a prediction plot.


```{r, warning=FALSE, echo = FALSE, fig.width=8}

data$predictIntercept = predict(glmerAllBuzzIntercept, type = "response")

## Below we calculate the uniform residuals

Zall = list(NULL)
m = 1
#for(k in unique(data$Ind)){
for (k in unique(data$Ind)){
  datak = data[data$Ind == k,]
  n     = length(datak$Buzz)
  Z     = NULL  ## Uniform residuals
  Buzzindices = (1:n)[datak$Buzz == 1]
  nB          = length(Buzzindices)
  dataki      = datak[1:Buzzindices[1],]
  Z    = c(Z,(exp(-sum(dataki$predictIntercept))))
  for(i in 2:nB){
      dataki = datak[(Buzzindices[i-1]+1):(Buzzindices[i]),]
      Z    = c(Z, (1-exp(-sum(dataki$predictIntercept))))
  }
  dataki = datak[Buzzindices[nB]:n,]
  Z      = c(Z,(exp(-sum(dataki$predictIntercept))))
  Zall[[m]] = list(Z = Z, Ind = k)
  m = m + 1
}



## Collect the uniform residuals in a data.frame
Zdata = data.frame(Z = Zall[[1]]$Z, Ind = Zall[[1]]$Ind, 
                   n = length(Zall[[1]]$Z))
for(i in 2:6){
  Zdata = rbind(Zdata, 
                data.frame(Z = Zall[[i]]$Z, Ind = Zall[[i]]$Ind, 
                           n = length(Zall[[i]]$Z)))
}

nn = length(Zdata$Z)

temp = acf(Zdata$Z, plot = FALSE)

acfplotdata = data.frame(acf = temp$acf, lag = temp$lag)

ggplot(data = acfplotdata, aes(x = lag, y = acf)) +
  geom_col(size = 0.2) + coord_fixed(36) +
  ylab("Autocorrelation of uniform residuals") +
  xlab("Lag")

#ggsave("Zacf.png", width = 10, height = 8, units = "cm")

zplotdata = data.frame(Zlow = Zdata$Z[1:(nn-1)], Zupp = Zdata$Z[2:nn])

ggplot(data = zplotdata, aes(x = Zlow, y = Zupp)) +
  geom_point(size = 0.5) + coord_fixed(1) +
  xlab(expression(Z[i-1])) +
  ylab(expression(Z[i])) +
  geom_abline(slope = 1, intercept = 0, size = 1)
#ggsave("Zplot.png", width = 10, height = 8, units = "cm")
```


QQplot for a given depth

```{r, warning=FALSE, echo = FALSE, cache = TRUE, fig.width=8}
Zall = list(NULL)
m = 1
for(k in unique(data$Ind)){
  datak = data[data$Ind == k,]
  n     = length(datak$Buzz)
  Z     = NULL  ## Uniform residuals
  Depthk = NULL
  Xk = NULL
  Pk = NULL
  Buzzindices = (1:n)[datak$Buzz == 1]
  nB          = length(Buzzindices)
  for(i in 2:nB){
      dataki = datak[(Buzzindices[i-1]+1):(Buzzindices[i]),]
      Z    = c(Z, (1-exp(-sum(dataki$predictIntercept))))
      Depthk = c(Depthk, dataki$Depth[1])
      Xk     = c(Xk, dataki$X[1])
      Pk     = c(Pk, dataki$P[1])
  }
 Zall[[m]] = list(Z = Z, Ind = k, X = Xk, P = Pk , Depth = Depthk)
  m = m + 1
}


## Collect the uniform residuals in a data.frame
Zdata = data.frame(Z = Zall[[1]]$Z, 
                   Ind = Zall[[1]]$Ind, 
                   n = length(Zall[[1]]$Z), 
                   Depth = Zall[[1]]$Depth,
                   X = Zall[[1]]$X, P = Zall[[1]]$P)
for(i in 2:6){
  Zdata = rbind(Zdata, 
                data.frame(Z = Zall[[i]]$Z, 
                   Ind = Zall[[i]]$Ind, 
                   n = length(Zall[[i]]$Z), 
                   Depth = Zall[[i]]$Depth,
                   X = Zall[[i]]$X, P = Zall[[i]]$P))
}

ZdataDepthbt = Zdata[-Zdata$Depth>400 & Zdata$X==0 &Zdata$P==0, ]
#ZdataDepthbt = Zdata[ Zdata$X==0 &Zdata$P==0, ]
nresid = length(ZdataDepthbt$Z)
Zorder = ZdataDepthbt$Z[order(ZdataDepthbt$Z)]
plot((1:nresid)/nresid, Zorder)
abline(a=0,b=1)



```





```{r, warning=FALSE, echo = FALSE, cache = TRUE, fig.width=8}
Intensityall = list(NULL)
scale = 3600/6
m = 1
for(k in unique(data$Ind)){
  datak     = data[data$Ind == k,]
  n         = dim(datak)[1]
  nBuzz     = NULL
  Intensity = NULL
  Depthk    = NULL
  Xk        = NULL
  Pk        = NULL
  Expmax    = NULL
  Expmin    = NULL
  for (i in 1:floor(n/scale)){
    dataki      = datak[(i-1)*scale+1:scale,]
    Buzzindices = (1:scale)[dataki$Buzz == 1]
    nBuzz       = c(nBuzz, length(Buzzindices))
    Intensity   = c(Intensity, sum(dataki$predictIntercept))
    Depthk      = c(Depthk, dataki$Depth[1])
    Xk          = c(Xk, mean(dataki$X))
    Pk          = c(Pk, mean(dataki$P))
    Expmax      = c(Expmax, ifelse(mean(dataki$X)<0.05 &mean(dataki$P)<0.05, 0, 50)) 
    Expmin      = c(Expmin, 0)
}
 Intensityall[[m]] = list(nBuzz = nBuzz, Ind = k, Intensity = Intensity, X = Xk, P = Pk, Expmax = Expmax, Expmin= Expmin, Depth = Depthk)
  m = m + 1
}


## Collect the uniform residuals in a data.frame
Intensitydata = data.frame(nBuzz = Intensityall[[1]]$nBuzz, 
                   Ind = Intensityall[[1]]$Ind, 
                   Intensity = Intensityall[[1]]$Intensity, 
                   Time = 1:length(Intensityall[[1]]$nBuzz), 
                   Depth = Intensityall[[1]]$Depth,
                   X = Intensityall[[1]]$X*75, 
                   P = Intensityall[[1]]$P*75,
                   ExposureMax = Intensityall[[1]]$Expmax,
                   ExposureMin = Intensityall[[1]]$Expmin)

for(i in 2:6){
  Intensitydata = rbind(Intensitydata, 
                data.frame(nBuzz = Intensityall[[i]]$nBuzz, 
                   Ind = Intensityall[[i]]$Ind, 
                   Intensity = Intensityall[[i]]$Intensity, 
                   Time = 1:length(Intensityall[[i]]$nBuzz), 
                   Depth = Intensityall[[i]]$Depth,
                   X = Intensityall[[i]]$X*75, 
                   P = Intensityall[[i]]$P*75,
                   ExposureMax = Intensityall[[i]]$Expmax,
                   ExposureMin = Intensityall[[i]]$Expmin))
}
 




ggplot(Intensitydata, aes(x=Time, y=Intensity, group=Ind)) + 
   geom_ribbon(aes(ymin=ExposureMin, ymax=ExposureMax), fill = "#FFFFCC") + 
geom_line()+xlab("Hour")+ylim(0,50)+xlim(0,750)+
  geom_point(aes(x=Time, y=nBuzz), size=0.1, color="red")+
   facet_wrap(~Ind)
#ggsave("BuzzCheckPredict.pdf") 

```

<!-- ## Simulation to validate the estimated rates -->

<!-- #### Model with offset equal to autoregressive coefficients (intercept re-estimated) -->

<!-- Simulation of several trajectories, starting with history $Y_{-p}=Y_{-p+1}=\ldots=Y_{-2}=0, Y_{-1}=0$ until the event $Y_{\tau-p+1}=Y_{\tau-p+2}=\ldots=Y_\tau=0$ with the stopping time -->
<!-- $$\tau = min\{ t>p, Y_{\tau-p+1}=Y_{\tau-p+2}=\ldots=Y_\tau=0\} $$ -->
<!-- Then we plot the histogram of $\tau$ and the histogram of $N(\tau)$, that is counting the number of 1 during the interval $[0, \tau]$.  -->


<!-- ```{r, warning=FALSE, echo = FALSE, cache = TRUE} -->
<!-- delta  = 1 # time step -->
<!-- # individual intercept are collected in coefficients(glmerAllBuzzIntercept)$Ind[,1] -->
<!-- # we take the fourth whale -->
<!-- alpha = coefficients(glmerAllBuzz)$Ind[4,1] #+ 0.01 *coefficients(glmerBuzzFinal)$Ind[5,2]  # intercept -->
<!-- p     = 63 # number of memory components -->
<!-- b1    = -7.76826061   -->
<!-- b2    = 0.30956649   -->
<!-- b3    = 0.95340294   -->
<!-- b4    = 0.010943783 -->
<!-- ARvec = b1*exp(-b2*(1:p))+b3*exp(-b4*(1:p)) # coefficients of the memory -->

<!-- Nrep = 100 #number of repetitions -->
<!-- tau  = NULL # stopping times -->
<!-- Ntau = NULL # number of buzzes in the sequence -->
<!-- for (i in 1:Nrep){ -->
<!--   # initialisation of the sequence -->
<!--   sim.seq.buzz = c(rep(0,p), 1) -->
<!--   t            = p+1 -->
<!--   while (sum(sim.seq.buzz[(t-p+1):t])>0){ -->
<!--     memory       = sum(sim.seq.buzz[(t-p+1):t]*ARvec) # autoregressive component -->
<!--     lambda       = exp(alpha + memory)                # intensity -->
<!--     sim.seq.buzz = c(sim.seq.buzz, rbinom(1,1,prob=min(1,lambda*delta))) -->
<!--     t = t+1 -->
<!--   } -->
<!--   tau  = c(tau, t) -->
<!--   Ntau = c(Ntau, sum(sim.seq.buzz)) -->
<!-- } -->

<!-- par(mfrow=c(1,2)) -->
<!-- hist(tau) -->
<!-- hist(Ntau) -->

<!-- ``` -->

<!-- We simulate a long trajectory of buzz, starting with history $Y_{-p}=Y_{-p+1}=\ldots=Y_{-2}=0, Y_{-1}=0$ on the interval $[0,100 000]$. Then we calculate the rate of buzzes per min and the number of buzzes.  -->
<!-- ```{r, warning=FALSE, echo = FALSE, cache = TRUE} -->
<!-- # one long simulation -->
<!-- # initialisation of the sequence -->
<!-- length.traj = 100000 -->
<!-- big.seq.buzz = c(rep(0,p), 1) -->
<!-- for (j in 1:length.traj){ -->
<!--   t            = p+j -->
<!--   memory       = sum(big.seq.buzz[(t-p+1):t]*ARvec) # autoregressive component -->
<!--   lambda       = exp(alpha + memory)                # intensity -->
<!--   big.seq.buzz = c(big.seq.buzz, rbinom(1,1,prob=min(1,lambda*delta))) -->

<!-- } -->
<!-- sum(big.seq.buzz)/length(big.seq.buzz)*60 -->


<!-- ``` -->



<!-- #### Model with offset equal to the (estimated) intercept + autoregressive coefficients  -->

<!-- Simulation of several trajectories, starting with history $Y_{-p}=Y_{-p+1}=\ldots=Y_{-2}=0, Y_{-1}=0$ until the event $Y_{\tau-p+1}=Y_{\tau-p+2}=\ldots=Y_\tau=0$ with the stopping time -->
<!-- $$\tau = min\{ t>p, Y_{\tau-p+1}=Y_{\tau-p+2}=\ldots=Y_\tau=0\} $$ -->
<!-- Then we plot the histogram of $\tau$ and the histogram of $N(\tau)$, that is counting the number of 1 during the interval $[0, \tau]$.  -->


<!-- ```{r, warning=FALSE, echo = FALSE, cache = TRUE} -->
<!-- delta  = 1 # time step -->
<!-- # individual intercept are collected in IndIntercept+coefficients(glmerAllBuzzIntercept)$Ind[,1] -->
<!-- # we take the fourth whale -->

<!-- alpha = IndIntercept[4]+coefficients(glmerAllBuzzIntercept)$Ind[4,1] #+ 0.01 *coefficients(glmerBuzzFinal)$Ind[5,2]  # intercept -->
<!-- p     = 63 # number of memory components -->
<!-- b1    = -7.76826061   -->
<!-- b2    = 0.30956649   -->
<!-- b3    = 0.95340294   -->
<!-- b4    = 0.010943783 -->
<!-- ARvec = b1*exp(-b2*(1:p))+b3*exp(-b4*(1:p)) # coefficients of the memory -->

<!-- Nrep = 100 #number of repetitions -->
<!-- tau  = NULL # stopping times -->
<!-- Ntau = NULL # number of buzzes in the sequence -->
<!-- for (i in 1:Nrep){ -->
<!--   # initialisation of the sequence -->
<!--   sim.seq.buzz = c(rep(0,p), 1) -->
<!--   t            = p+1 -->
<!--   while (sum(sim.seq.buzz[(t-p+1):t])>0){ -->
<!--     memory       = sum(sim.seq.buzz[(t-p+1):t]*ARvec) # autoregressive component -->
<!--     lambda       = exp(alpha + memory)                # intensity -->
<!--     sim.seq.buzz = c(sim.seq.buzz, rbinom(1,1,prob=min(1,lambda*delta))) -->
<!--     t = t+1 -->
<!--   } -->
<!--   tau  = c(tau, t) -->
<!--   Ntau = c(Ntau, sum(sim.seq.buzz)) -->
<!-- } -->

<!-- par(mfrow=c(1,2)) -->
<!-- hist(tau) -->
<!-- hist(Ntau) -->

<!-- ``` -->

<!-- We simulate a long trajectory of buzz, starting with history $Y_{-p}=Y_{-p+1}=\ldots=Y_{-2}=0, Y_{-1}=0$ on the interval $[0,100 000]$. Then we calculate the rate of buzzes per min and the number of buzzes.  -->
<!-- ```{r, warning=FALSE, echo = FALSE, cache = TRUE} -->
<!-- # one long simulation -->
<!-- # initialisation of the sequence -->
<!-- length.traj = 100000 -->
<!-- big.seq.buzz = c(rep(0,p), 1) -->
<!-- for (j in 1:length.traj){ -->
<!--   t            = p+j -->
<!--   memory       = sum(big.seq.buzz[(t-p+1):t]*ARvec) # autoregressive component -->
<!--   lambda       = exp(alpha + memory)                # intensity -->
<!--   big.seq.buzz = c(big.seq.buzz, rbinom(1,1,prob=min(1,lambda*delta))) -->

<!-- } -->
<!-- sum(big.seq.buzz)/length(big.seq.buzz)*60 -->


<!-- ``` -->

\newpage

## 3. Model including depth in glm and as offset in glmer



We fitted a memory component to buzzes with lags from 1 to $k$, where we varied $k$ between 1 and 100, adjusted to Depth, entered as a nonlinear spline function. For each model, we computed the BIC and chose the model with the lowest BIC. Below, only a subset of k are chosen. This can be changed as desired in the definition of lagvector.


```{r, warning=FALSE, echo = FALSE, cache = TRUE}
data <- dataAll

## Time provides time since tagging
data$Time <- numeric()
for(ind in unique(data$Ind)){
  indicator <- (data$Ind == ind)
  data$Time[indicator] <- TimeFunction(n = sum(indicator))
}

## Define the first 100 time lags
temp <- as.data.frame(shift(data$Buzz, n = 1:100, give.names = TRUE))
data <- cbind(temp, data)

## keep the 2018 data
data = data[data$Year=="2018",]


## Find times of first buzz and first exposure time and 
## restrict data to be between these for each whale
for(k in unique(data$Ind)){
  mintime <- min(data$Time[(data$Ind == k) & (data$Buzz == 1)])
  maxtime <- min(data$Time[(data$Ind == k) & (data$LOS == 1)])
  data <- subset(data, !(Ind == k & Time > maxtime))
  data <- subset(data, !(Ind == k & Time < mintime))
}


```

```{r, warning=FALSE, echo = FALSE, cache = TRUE}
LagVariables <- names(data[,1:100])

glmresultsDepth <- list(NULL)

lagvector <- c(50,seq(55,65,1))

n <- length(lagvector)
AICvector <- numeric(n) 
BICvector <- numeric(n) 
for(i in 1:n){
  form <- paste("Buzz ~ Ind + ns(Depth, knots = c(-323, -158, -54)) + ",
                paste(LagVariables[1:lagvector[i]], collapse = " + "))
  
  glmAllBuzzDepth <- glm(form, 
                    data = data, 
                    family = poisson)
  
  AIC <- AIC(glmAllBuzzDepth)
  BIC <- BIC(glmAllBuzzDepth)
  
  AICvector[i]    <- AIC
  BICvector[i]    <- BIC
  coeftable <- coefficients(summary(glmAllBuzzDepth))
  glmresultsDepth[[i]] <- list(lag = lagvector[i], coeftable = coeftable, AIC = AIC, BIC = BIC)
}
```

The following figure shows the BIC values for different maximum lag values. The minimum BIC is for maximum lag = 60.

```{r, warning=FALSE, echo = FALSE, cache = TRUE, fig.width=8}
dataBICDepth <- data.frame(lag = lagvector, BIC = BICvector)

ggplot(dataBICDepth, aes(x = lag, y = BIC)) +
  geom_line() +
  geom_vline(xintercept = 60, col = "blue", lty = 2) +
  xlab("Maximum lag")
```


#### Estimation of the AR coefficients.

```{r, warning=FALSE, echo = FALSE, cache = TRUE, fig.width=8}
k <- length(glmresultsDepth)
kvector <- 1:12#c(2:6,16:19)

temp    <- glmresultsDepth[[1]]
ARcoef  <- temp$coeftable[-(1:10), "Estimate"]
lag     <- temp$lag

fitresult <- data.frame(maxlag = as.factor(rep(lag, lag)), lag = 1:lag,
                        estimate = ARcoef)

curvefit  <- data.frame(maxlag = as.factor(rep(lag, lag)), lag = 1:lag,
                        b1 = rep(NA, lag), b2 = rep(NA, lag), 
                        b3 = rep(NA, lag), b4 = rep(NA, lag))

for(i in kvector){
  temp    <- glmresultsDepth[[i]]
  lag     <- temp$lag
  ARcoef  <- temp$coeftable[-(1:10), "Estimate"]
  std.err <- temp$coeftable[-(1:10),"Std. Error"]
  w       <- 1/std.err
  n       <- length(ARcoef)
  
  fm <- nls(ARcoef ~ SSbiexp(1:n, A1, lrc1, A2, lrc2), weights = w)
  b1 <- summary(fm)$coefficients[1,1]
  b2 <- exp(summary(fm)$coefficients[2,1])
  b3 <- summary(fm)$coefficients[3,1]
  b4 <- exp(summary(fm)$coefficients[4,1])
  fitresulti <- data.frame(maxlag = as.factor(rep(lag, lag)), lag = 1:lag, 
                           estimate = ARcoef)
  curvefiti  <- data.frame(maxlag = as.factor(rep(lag, lag)), lag = 1:lag,
                           b1 = rep(b1, lag), b2 = rep(b2, lag), 
                           b3 = rep(b3, lag), b4 = rep(b4, lag))
  fitresult <- rbind(fitresult, fitresulti)
  curvefit  <- rbind(curvefit, curvefiti)
}

curvefit$estimate <- curvefit$b1 * exp(-curvefit$b2 * curvefit$lag) +
  curvefit$b3 * exp(-curvefit$b4 * curvefit$lag)

ggplot(fitresult, aes(x = lag, y = estimate, color = maxlag)) +
  geom_point(alpha = 0.3) +
  geom_line(aes(x = lag, y = estimate, color = maxlag), data = curvefit) +
  geom_hline(yintercept = 0) + 
  theme(legend.position="top")


```

#### Estimation of the exposure effect

```{r, warning=FALSE, echo = FALSE, cache = TRUE}
data <- dataAll

## Time provides time since tagging
data$Time <- numeric()
for(ind in unique(data$Ind)){
  indicator <- (data$Ind == ind)
  data$Time[indicator] <- TimeFunction(n = sum(indicator))
}

## Define the first 100 time lags
temp <- as.data.frame(shift(data$Buzz, n = 1:100, give.names = TRUE))
data <- cbind(temp, data)


## We cut the first 24 hours of all individuals to avoid
## the effect of tagging, i.e., cut 43200 data points
data <- data[data$Time > 24,]

## Defining exposures
data$Xtilde <- ExposureFunction(X = data$Dist_Ship) ## Exposure variable based on 1/distance in km

data$X = data$Xtilde*(data$Seismik == 1)
data$P = data$Xtilde*(data$Seismik == 0)

## Only data from airgun or no exposure at all (removing ship):
data <- subset(data, P == 0)

## Only data from 2018
data = data[data$Year=="2018",]

plotdist <- seq(2,50,0.1) # distances for plotting in km

```


The generalized linear mixed model with a Poisson response distribution with a log-link to model the effect of exposure on buzzing rate (buzzes/min) with an autoregressive memory component. Exposure is defined as 1/distance (km).Exposure is entered non-linearly as an explanatory variable using natural cubic splines with 3 degrees of freedom (ns, package splines) with internal knots located at the 33th and 66th percentiles of the non-zero exposure values. Individual is included as a random effect allowing each animal to have a unique baseline (intercept) in their sound production rate. To obtain convergence the optimization is done by Adaptive Gauss-Hermite Quadrature, which is obtained by the option nAGQ = 0 in the glmer-call. The default is nAGQ = 1, the Laplace approximation, which does not reach convergence.


```{r, warning=FALSE, echo = FALSE, cache = TRUE}


print("Coefficients in memory kernel")
k=60; i = 7
temp    <- glmresultsDepth[[i]]
lag     <- temp$lag
ARcoef  <- temp$coeftable[-(1:10), "Estimate"]
std.err <- temp$coeftable[-(1:10),"Std. Error"]
w       <- 1/std.err
n       <- length(ARcoef)
  
fm <- nls(ARcoef ~ SSbiexp(1:n, A1, lrc1, A2, lrc2), weights = w)
b1 <- summary(fm)$coefficients[1,1]
b2 <- exp(summary(fm)$coefficients[2,1])
b3 <- summary(fm)$coefficients[3,1]
b4 <- exp(summary(fm)$coefficients[4,1])

#b1 = -7.755725  
#b2 = 0.3487828  
#b3 = 0.6565633  
#b4 = 0.00988948
ARvec <- b1*exp(-b2*(1:60))+b3*exp(-b4*(1:60))
LagVariables <- names(data[,1:60])
dataAR  <- data[,LagVariables]
## Autoregressive component for offset in later analyses
data$ARDepth <- as.matrix(dataAR) %*% ARvec
## Depth coefficients for offset
Depthcoeff <- temp$coeftable[7:10,"Estimate"]
data$ARDepth <- data$ARDepth +as.matrix(ns(data$Depth, knots = c(-323, -158, -54)))%*% Depthcoeff

## Weights for the glmer analysis
data$n <- rep(0, length(data$Ind))
for(k in unique(data$Ind)){
  data$n[data$Ind == k] <- length(data$Ind[data$Ind == k])
}

glmerAllBuzzDepth <- glmer(Buzz ~ offset(ARDepth) + #ns(Depth, knots = c(-323, -158, -54)) + 
                        ns(X, knots = quantile(data$X[data$X > 0],c(1:2)/3)) +
                        (1 | Ind),
                      data = data,
                      nAGQ=0,
                      weights = n,
                      family = poisson)

summary(glmerAllBuzzDepth)
```

#### Comparison of the estimated buzzing rates with or without Depth

```{r, warning=FALSE, echo = FALSE, cache = TRUE, fig.width=8}
plotdist <- seq(1,70,0.1) ## Plotting distances in km
# individual prediction wrt exposure X for glmerAllBuzz
predFrameMany <- NULL
Depthlevel = -400
for(k in unique(data$Ind)){
  temp <- range(data$X[data$X > 0 & data$Ind == k])
  temp1 <- expand.grid(X = 1/seq(1/temp[2],1/temp[1],0.1),
                       P = 0,
                       Depth = Depthlevel,
                       AR = 0,
                       Ind = k)
  predFrameMany <- rbind(predFrameMany, temp1)
}
predBuzz <- predict(glmerAllBuzz,
                    newdata = predFrameMany)
predFrame <- cbind(predFrameMany, as.data.frame(predBuzz))
predFrame$exposure = "Trial"
predFrame$model = "Without Depth"

# individual prediction wrt exposure X for glmerAllBuzzDepth
predFrameMany <- NULL
for(k in unique(data$Ind)){
  temp <- range(data$X[data$X > 0 & data$Ind == k])
  temp1 <- expand.grid(X = 1/seq(1/temp[2],1/temp[1],0.1),
                       P = 0,
                       Depth = Depthlevel,
                       ARDepth = 0  ,
                       Ind = k)
  predFrameMany <- rbind(predFrameMany, temp1)
}
predBuzz <- predict(glmerAllBuzzDepth,
                    newdata = predFrameMany)
temp <- cbind(predFrameMany, as.data.frame(predBuzz))
temp$exposure = "Trial"
temp$model = "With Depth"
names(temp) <- c("X" ,"P", "Depth", "AR", "Ind","predBuzz", "exposure" ,"model")
predFrame <- rbind(predFrame, temp)

# population prediction wrt exposure X for glmerAllBuzz
predFramePop_temp <- expand.grid(X = 1/plotdist,
                            P = 0,
                             Depth = Depthlevel,
                            AR = 0,
                            Ind = "Population")
predBuzzPop <- predict(glmerAllBuzz,
                    newdata = predFramePop_temp,
                    re.form = NA)
predFramePop <- cbind(predFramePop_temp, as.data.frame(predBuzzPop))
predFramePop$model = "Without Depth"
# population prediction wrt exposure X for glmerAllBuzzDepth
predFramePop_temp <- expand.grid(X = 1/plotdist,
                            P = 0,
                             Depth = Depthlevel,
                            ARDepth = 0,
                            Ind = "Population")
predBuzzPop <- predict(glmerAllBuzzDepth,
                    newdata = predFramePop_temp,
                    re.form = NA)
temp <- cbind(predFramePop_temp, as.data.frame(predBuzzPop))
temp$model = "With Depth"
names(temp) <- c("X" ,"P", "Depth", "AR", "Ind","predBuzzPop", "model")
predFramePop = rbind(predFramePop, temp)

# individual prediction with no exposure X for glmerAllBuzz
predFrame0_temp <- expand.grid(X = 0,
                          P = 0,
                          Depth = Depthlevel,
                          AR = 0,
                          Ind = unique(data$Ind))
predBuzz0 <- predict(glmerAllBuzz,
                    newdata = predFrame0_temp)
predFrame0 <- cbind(predFrame0_temp, as.data.frame(predBuzz0))
predFrame0$model = "Without Depth"
# individual prediction with no exposure X for glmerAllBuzzDepth
predFrame0_temp <- expand.grid(X = 0,
                          P = 0,
                          Depth = Depthlevel,
                          ARDepth = 0,
                          Ind = unique(data$Ind))
predBuzz0 <- predict(glmerAllBuzzDepth,
                    newdata = predFrame0_temp)
temp <- cbind(predFrame0_temp, as.data.frame(predBuzz0))
temp$model = "With Depth"
names(temp) <- c("X" ,"P",  "Depth", "AR", "Ind","predBuzz0", "model")
predFrame0 <- rbind(predFrame0, temp)
#fit0 <- predFrame0$predBuzz0

# population prediction with no exposure X for glmerAllBuzz
predFramePop0_temp <- expand.grid(X = 0,
                             P = 0,
                              Depth = Depthlevel,
                              AR = 0,
                             Ind = "Population")
predBuzzPop0 <- predict(glmerAllBuzz,
                    newdata = predFramePop0_temp,
                    re.form = NA)
predFramePop0 <- cbind(predFramePop0_temp, as.data.frame(predBuzzPop0))
predFramePop0$model = "Without Depth"
fitPop0 <- predFramePop0$predBuzzPop0
# population prediction with no exposure X for glmerAllBuzzDepth
predFramePop0_temp <- expand.grid(X = 0,
                             P = 0,
                              Depth = Depthlevel,
                              ARDepth = 0,
                             Ind = "Population")
predBuzzPop0 <- predict(glmerAllBuzzDepth,
                    newdata = predFramePop0_temp,
                    re.form = NA)
temp <- cbind(predFramePop0_temp, as.data.frame(predBuzzPop0))
temp$model = "With Depth"
names(temp) <- c("X" ,"P",  "Depth", "AR", "Ind","predBuzzPop0", "model")
predFramePop0 <- rbind(predFramePop0, temp)


ggplot(data = predFrame, aes(x=1/X, y=60*exp(predBuzz), color = Ind)) +
  geom_line() +
  geom_line(data = predFramePop, aes(x=1/X, y=60*exp(predBuzzPop)), size = 1.2, color = "black") +
  #ylim(0, 0.3) +
  xlim(0, 50) +
  xlab("Distance to ship (km)") +
  ylab("Buzzing rate (1/min)") +
  #geom_hline(data = predFrame0, aes(x = 1/X), yintercept = exp(predBuzz0)*60, color = "gray")  +
  #geom_hline(data = predFramePop0, aes(x = 1/X), yintercept = exp(predBuzzPop0)*60, color = "black", size = 1.2) +
# geom_hline(yintercept = exp(fit0)*60, color = "gray")  +
 #geom_hline(yintercept = exp(fitPop0)*60, color = "black", size = 1.2) +
  facet_grid(. ~ model)

#ggsave("BuzzingRateInd.pdf")

```

#### Percentage of normal behavior

```{r, warning=FALSE, echo = FALSE, cache = TRUE, fig.width=8}

# percentage of normal behavior
ChangePop = predFramePop
ChangePop$change = exp(ChangePop$predBuzzPop)
ChangePop$change[1:length(plotdist)] = ChangePop$change[1:length(plotdist)]/exp(predFramePop0$predBuzzPop0[1])*100
ChangePop$change[(length(plotdist)+1):(2*length(plotdist))] = ChangePop$change[(length(plotdist)+1):(2*length(plotdist))]/exp(predFramePop0$predBuzzPop0[2])*100


  ggplot(data = ChangePop,
       aes(x = change, y = 1/X, color = model)) +
  geom_line() +
  ylim(0,40) + ylab("Distance to ship") +
  xlab("Percentage of normal buzzing rate") +
   #  facet_grid(~ model)  + 
    theme(legend.position = "top", legend.title = element_blank())
  ggsave("PercentageNormalBehavior.pdf")
```

#### Model control for model with offset including AR +Depth   

We will do model control on our final model glmerAllBuglmerAllBuzzDepthzzIntercept.

We calculate uniform residuals (explained in the supplementary material) and make several plots: an autocorrelation plot that shows that residuals are approximately non-correlated,  a plot of residual $i$ against residual $i-1$ to see if any (unwanted) structure emerges, a qqplot and a prediction plot.


```{r, warning=FALSE, echo = FALSE, cache = TRUE, fig.width=8}

data$predictDepth = predict(glmerAllBuzzDepth, type = "response")

## Below we calculate the uniform residuals

Zall = list(NULL)
m = 1
#for(k in unique(data$Ind)){
for (k in unique(data$Ind)){
  datak = data[data$Ind == k,]
  n     = length(datak$Buzz)
  Z     = NULL  ## Uniform residuals
  Buzzindices = (1:n)[datak$Buzz == 1]
  nB          = length(Buzzindices)
  dataki      = datak[1:Buzzindices[1],]
  Z    = c(Z,(exp(-sum(dataki$predictDepth))))
  for(i in 2:nB){
      dataki = datak[(Buzzindices[i-1]+1):(Buzzindices[i]),]
      Z    = c(Z, (1-exp(-sum(dataki$predictDepth))))
  }
  dataki = datak[Buzzindices[nB]:n,]
  Z      = c(Z,(exp(-sum(dataki$predictDepth))))
  Zall[[m]] = list(Z = Z, Ind = k)
  m = m + 1
}



## Collect the uniform residuals in a data.frame
Zdata = data.frame(Z = Zall[[1]]$Z, Ind = Zall[[1]]$Ind, 
                   n = length(Zall[[1]]$Z))
for(i in 2:6){
  Zdata = rbind(Zdata, 
                data.frame(Z = Zall[[i]]$Z, Ind = Zall[[i]]$Ind, 
                           n = length(Zall[[i]]$Z)))
}

nn = length(Zdata$Z)

temp = acf(Zdata$Z, plot = FALSE)

acfplotdata = data.frame(acf = temp$acf, lag = temp$lag)

ggplot(data = acfplotdata, aes(x = lag, y = acf)) +
  geom_col(size = 0.2) + coord_fixed(36) +
  ylab("Autocorrelation of uniform residuals") +
  xlab("Lag")

#ggsave("Zacf.png", width = 10, height = 8, units = "cm")

zplotdata = data.frame(Zlow = Zdata$Z[1:(nn-1)], Zupp = Zdata$Z[2:nn])

ggplot(data = zplotdata, aes(x = Zlow, y = Zupp)) +
  geom_point(size = 0.5) + coord_fixed(1) +
  xlab(expression(Z[i-1])) +
  ylab(expression(Z[i])) +
  geom_abline(slope = 1, intercept = 0, size = 1)
#ggsave("Zplot.png", width = 10, height = 8, units = "cm")
```


QQplot for a given depth

```{r, warning=FALSE, echo = FALSE, cache = TRUE, fig.width=8}
Zall = list(NULL)
m = 1
for(k in unique(data$Ind)){
  datak = data[data$Ind == k,]
  n     = length(datak$Buzz)
  Z     = NULL  ## Uniform residuals
  Depthk = NULL
  Xk = NULL
  Pk = NULL
  Buzzindices = (1:n)[datak$Buzz == 1]
  nB          = length(Buzzindices)
  for(i in 2:nB){
      dataki = datak[(Buzzindices[i-1]+1):(Buzzindices[i]),]
      Z    = c(Z, (1-exp(-sum(dataki$predictDepth))))
      Depthk = c(Depthk, dataki$Depth[1])
      Xk     = c(Xk, dataki$X[1])
      Pk     = c(Pk, dataki$P[1])
  }
 Zall[[m]] = list(Z = Z, Ind = k, X = Xk, P = Pk , Depth = Depthk)
  m = m + 1
}


## Collect the uniform residuals in a data.frame
Zdata = data.frame(Z = Zall[[1]]$Z, 
                   Ind = Zall[[1]]$Ind, 
                   n = length(Zall[[1]]$Z), 
                   Depth = Zall[[1]]$Depth,
                   X = Zall[[1]]$X, P = Zall[[1]]$P)
for(i in 2:6){
  Zdata = rbind(Zdata, 
                data.frame(Z = Zall[[i]]$Z, 
                   Ind = Zall[[i]]$Ind, 
                   n = length(Zall[[i]]$Z), 
                   Depth = Zall[[i]]$Depth,
                   X = Zall[[i]]$X, P = Zall[[i]]$P))
}

ZdataDepthbt = Zdata[-Zdata$Depth>400 & Zdata$X==0 &Zdata$P==0, ]
#ZdataDepthbt = Zdata[ Zdata$X==0 &Zdata$P==0, ]
nresid = length(ZdataDepthbt$Z)
Zorder = ZdataDepthbt$Z[order(ZdataDepthbt$Z)]

df = data.frame(qunif = (1:nresid)/nresid,
                qZ = Zorder)

ggplot(df, aes(x=qunif, y=Zorder))+
  geom_point()+
  xlab("Uniform theoretical quantiles")+
  ylab("Empirical quantiles")
#ggsave("QuantilesZ.png", width = 10, height = 8, units = "cm")

```


Figure 7

```{r, warning=FALSE, echo = FALSE, cache = TRUE, fig.width=8}
Intensityall = list(NULL)
scale = 3600/6
m = 1
for(k in unique(data$Ind)){
  datak     = data[data$Ind == k,]
  n         = dim(datak)[1]
  nBuzz     = NULL
  Intensity = NULL
  Depthk    = NULL
  Xk        = NULL
  Pk        = NULL
  Expmax    = NULL
  Expmin    = NULL
  for (i in 1:floor(n/scale)){
    dataki      = datak[(i-1)*scale+1:scale,]
    Buzzindices = (1:scale)[dataki$Buzz == 1]
    nBuzz       = c(nBuzz, length(Buzzindices))
    Intensity   = c(Intensity, sum(dataki$predictDepth))
    Depthk      = c(Depthk, dataki$Depth[1])
    Xk          = c(Xk, mean(dataki$X))
    Pk          = c(Pk, mean(dataki$P))
    Expmax      = c(Expmax, ifelse(mean(dataki$X)<0.05 &mean(dataki$P)<0.05, 0, 50)) 
    Expmin      = c(Expmin, 0)
}
 Intensityall[[m]] = list(nBuzz = nBuzz, Ind = k, Intensity = Intensity, X = Xk, P = Pk, Expmax = Expmax, Expmin= Expmin, Depth = Depthk)
  m = m + 1
}


## Collect the uniform residuals in a data.frame
Intensitydata = data.frame(nBuzz = Intensityall[[1]]$nBuzz, 
                   Ind = Intensityall[[1]]$Ind, 
                   Intensity = Intensityall[[1]]$Intensity, 
                   Time = 1:length(Intensityall[[1]]$nBuzz), 
                   Depth = Intensityall[[1]]$Depth,
                   X = Intensityall[[1]]$X*75, 
                   P = Intensityall[[1]]$P*75,
                   ExposureMax = Intensityall[[1]]$Expmax,
                   ExposureMin = Intensityall[[1]]$Expmin)

for(i in 2:6){
  Intensitydata = rbind(Intensitydata, 
                data.frame(nBuzz = Intensityall[[i]]$nBuzz, 
                   Ind = Intensityall[[i]]$Ind, 
                   Intensity = Intensityall[[i]]$Intensity, 
                   Time = 1:length(Intensityall[[i]]$nBuzz), 
                   Depth = Intensityall[[i]]$Depth,
                   X = Intensityall[[i]]$X*75, 
                   P = Intensityall[[i]]$P*75,
                   ExposureMax = Intensityall[[i]]$Expmax,
                   ExposureMin = Intensityall[[i]]$Expmin))
}
 



ggplot(Intensitydata, aes(x=Time, y=Intensity, group=Ind)) + 
   geom_ribbon(aes(ymin=ExposureMin, ymax=ExposureMax), fill = "#FFFFCC") + 
geom_line()+xlab("Hour")+ylim(0,50)+xlim(0,750)+
  geom_point(aes(x=Time, y=nBuzz), size=0.1, color="red")+
   facet_wrap(~Ind)
ggsave("BuzzCheckPredict.pdf") 

```